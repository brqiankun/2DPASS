
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/2dpass-2d-priors-assisted-semantic/3d-semantic-segmentation-on-semantickitti)](https://paperswithcode.com/sota/3d-semantic-segmentation-on-semantickitti?p=2dpass-2d-priors-assisted-semantic)[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/2dpass-2d-priors-assisted-semantic/lidar-semantic-segmentation-on-nuscenes)](https://paperswithcode.com/sota/lidar-semantic-segmentation-on-nuscenes?p=2dpass-2d-priors-assisted-semantic)

# 2DPASS

[![arXiv](https://img.shields.io/badge/arXiv-2203.09065-b31b1b.svg)](https://arxiv.org/pdf/2207.04397.pdf)
[![GitHub Stars](https://img.shields.io/github/stars/yanx27/2DPASS?style=social)](https://github.com/yanx27/2DPASS)
![visitors](https://visitor-badge.glitch.me/badge?page_id=https://github.com/yanx27/2DPASS)



This repository is for **2DPASS** introduced in the following paper

[Xu Yan*](https://yanx27.github.io/), [Jiantao Gao*](https://github.com/Gao-JT), [Chaoda Zheng*](https://github.com/Ghostish), Chao Zheng, Ruimao Zhang, Shuguang Cui, [Zhen Li*](https://mypage.cuhk.edu.cn/academics/lizhen/), "*2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds*", ECCV 2022 [[arxiv]](https://arxiv.org/pdf/2207.04397.pdf).
 ![image](figures/2DPASS.gif)

If you find our work useful in your research, please consider citing:
```latex
@inproceedings{yan20222dpass,
  title={2dpass: 2d priors assisted semantic segmentation on lidar point clouds},
  author={Yan, Xu and Gao, Jiantao and Zheng, Chaoda and Zheng, Chao and Zhang, Ruimao and Cui, Shuguang and Li, Zhen},
  booktitle={European Conference on Computer Vision},
  pages={677--695},
  year={2022},
  organization={Springer}
}

@InProceedings{yan2022let,
      title={Let Images Give You More: Point Cloud Cross-Modal Training for Shape Analysis}, 
      author={Xu Yan and Heshen Zhan and Chaoda Zheng and Jiantao Gao and Ruimao Zhang and Shuguang Cui and Zhen Li},
      year={2022},
      booktitle={NeurIPS}
}

@article{yan2023benchmarking,
  title={Benchmarking the Robustness of LiDAR Semantic Segmentation Models},
  author={Yan, Xu and Zheng, Chaoda and Li, Zhen and Cui, Shuguang and Dai, Dengxin},
  journal={arXiv preprint arXiv:2301.00970},
  year={2023}
}
```
## News
* **2023-04-01** We merge MinkowskiNet and official SPVCNN models from [SPVNAS](https://github.com/mit-han-lab/spvnas) in our codebase. You can check these models in `config/`. We rename our baseline model from `spvcnn.py` to `baseline.py`.
* **2023-03-31** We provide codes for the robustness evaluation on SemanticKITTI-C.
* **2023-03-27** We release a model with higher performance on SemanticKITTI and codes for naive instance augmentation.
* **2023-02-25** We release a new robustness benchmark for LiDAR semantic segmentation at [SemanticKITTI-C](https://yanx27.github.io/RobustLidarSeg/). Welcome to test your models!
<p align="center">
   <img src="figures/semantickittic.png" width="80%"> 
</p>


* **2022-10-11** Our new work for cross-modal knowledge distillation is accepted at NeurIPS 2022:smiley: [paper](https://arxiv.org/pdf/2210.04208.pdf) / [code](https://github.com/ZhanHeshen/PointCMT).
* **2022-09-20** We release codes for SemanticKITTI single-scan and NuScenes :rocket:!
* **2022-07-03** 2DPASS is accepted at **ECCV 2022** :fire:!
* **2022-03-08** We achieve **1st** place in both single and multi-scans of [SemanticKITTI](http://semantic-kitti.org/index.html) and **3rd** place on [NuScenes-lidarseg](https://www.nuscenes.org/) :fire:! 
<p align="center">
   <img src="figures/singlescan.jpg" width="80%"> 
</p>
<p align="center">
   <img src="figures/multiscan.jpg" width="80%"> 
</p>
<p align="center">
   <img src="figures/nuscene.png" width="80%"> 
</p>

## Installation

### Requirements
- pytorch >= 1.8 
- yaml
- easydict  `conda install -c conda-forge easydict` done
- pyquaternion   å››å…ƒæ•°åº“ `conda install -c conda-forge quaternion`  this one `pip install pyquaternion` (http://kieranwynn.github.io/pyquaternion/)  done
- [lightning](https://github.com/Lightning-AI/lightning) (https://lightning.ai/docs/pytorch/latest/)  (tested with pytorch_lightning==1.3.8 and torchmetrics==0.5)  this one `pip install pytorch_lightning==1.3.8 pip install torchmetrics==0.5`  `conda install lightning -c conda-forge` done  
https://pytorch-lightning.readthedocs.io/en/1.3.8/api_references.html 
https://lightning.ai/docs/pytorch/LTS/past_versions.html
- [torch-scatter](https://github.com/rusty1s/pytorch_scatter) (pip install torch-scatter -f https://data.pyg.org/whl/torch-1.9.0+${CUDA}.html) 
  this one `conda install pytorch-scatter -c pyg`    done
- [nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit) `pip install nuscenes-devkit` done (optional for nuScenes)
- [spconv](https://github.com/traveller59/spconv) (tested with spconv==2.1.16 and cuda==11.1, pip install spconv-cu111==2.1.16) this one `pip install spconv-cu117`done 
- [torchsparse](https://github.com/mit-han-lab/torchsparse) (optional for MinkowskiNet and SPVCNN. sudo apt-get install libsparsehash-dev, pip install --upgrade git+https://github.com/mit-han-lab/torchsparse.git@v1.4.0)
- pip install -U tensorboard
- pip install -U tensorboardX

## Data Preparation

### SemanticKITTI
Please download the files from the [SemanticKITTI website](http://semantic-kitti.org/dataset.html) and additionally the [color data](http://www.cvlibs.net/download.php?file=data_odometry_color.zip) from the [Kitti Odometry website](http://www.cvlibs.net/datasets/kitti/eval_odometry.php). Extract everything into the same folder.
```
./dataset/
â”œâ”€â”€ 
â”œâ”€â”€ ...
â””â”€â”€ SemanticKitti/
    â”œâ”€â”€sequences
        â”œâ”€â”€ 00/           
        â”‚   â”œâ”€â”€ velodyne/	
        |   |	â”œâ”€â”€ 000000.bin
        |   |	â”œâ”€â”€ 000001.bin
        |   |	â””â”€â”€ ...
        â”‚   â””â”€â”€ labels/ 
        |   |   â”œâ”€â”€ 000000.label
        |   |   â”œâ”€â”€ 000001.label
        |   |   â””â”€â”€ ...
        |   â””â”€â”€ image_2/ 
        |   |   â”œâ”€â”€ 000000.png
        |   |   â”œâ”€â”€ 000001.png
        |   |   â””â”€â”€ ...
        |   calib.txt
        â”œâ”€â”€ 08/ # for validation
        â”œâ”€â”€ 11/ # 11-21 for testing
        â””â”€â”€ 21/
	    â””â”€â”€ ...
```

### NuScenes
Please download the Full dataset (v1.0) from the [NuScenes website](https://www.nuscenes.org/) with lidarseg and extract it.
```
./dataset/
â”œâ”€â”€ 
â”œâ”€â”€ ...
â””â”€â”€ nuscenes/
    â”œâ”€â”€v1.0-trainval
    â”œâ”€â”€v1.0-test
    â”œâ”€â”€samples
    â”œâ”€â”€sweeps
    â”œâ”€â”€maps
    â”œâ”€â”€lidarseg
```

## Training
### SemanticKITTI
è®­ç»ƒé›†å…±19130å¸§
You can run the training with
batch_size è®¾ç½®ä¸º2å¯ä»¥è®­ç»ƒï¼Œ 1ä¸ªepoch 2:40:00  
æ˜¾å­˜å ç”¨
7031MiB /  8188MiB
loss=12.9
```shell script
cd <root dir of this repo>
python main.py --log_dir 2DPASS_semkitti --config config/2DPASS-semantickitti.yaml --gpu 0

python main.py --log_dir 2DPASS_semkitti --config=./config/2DPASS-semantickitti.yaml --gpu 0 --save_top_k -1 --every_n_train_steps 500 --checkpoint=./checkpoint/best_model.ckpt

python main.py --log_dir 2DPASS_semkitti --config=./config/2DPASS-semantickitti.yaml --gpu 0 --fine_tune --save_top_k -1 --every_n_train_steps 500 --checkpoint=./checkpoint/pretrained/semantickitti/2DPASS_4scale_64dim/best_model.ckpt

python main.py --log_dir 2DPASS_rellis --config ./config/2DPASS-RELLIS-3D-kitti-format.yaml --gpu 0

```
The output will be written to `logs/SemanticKITTI/2DPASS_semkitti` by default. 

### Rellis-3d
è®­ç»ƒé›†å…±9313  [00000, 00001, 00002]


### NuScenes
```shell script
cd <root dir of this repo>
python main.py --log_dir 2DPASS_nusc --config config/2DPASS-nuscenese.yaml --gpu 0 1 2 3
```

### Vanilla Training without 2DPASS
We take SemanticKITTI as an example.
```shell script
cd <root dir of this repo>
python main.py --log_dir baseline_semkitti --config config/2DPASS-semantickitti.yaml --gpu 0 --baseline_only
```

## Testing
You can run the testing with
æ˜¾å­˜å ç”¨
2205MiB /  8188MiB
```shell script
cd <root dir of this repo>
python main.py --config config/2DPASS-semantickitti.yaml --gpu 0 --test --num_vote 12 --checkpoint <dir for the pytorch checkpoint>

python main.py --config config/2DPASS-semantickitti.yaml --gpu 0 --test --num_vote 1 --checkpoint checkpoint/best_model.ckpt

python main.py --config_path config/2DPASS-semantickitti.yaml --gpu 0 --test --num_vote 1 --checkpoint=./checkpoint/pretrained/semantickitti/2DPASS_4scale_64dim/best_model.ckpt
```
Here, `num_vote` is the number of views for the test-time-augmentation (TTA). We set this value to 12 as default (on a Tesla-V100 GPU), and if you use other GPUs with smaller memory, you can choose a smaller value. `num_vote=1` denotes there is no TTA used, and will cause about ~2\% performance drop.




## Robustness Evaluation
Please download all subsets of [SemanticKITTI-C](https://arxiv.org/pdf/2301.00970.pdf) from [this link](https://cuhko365-my.sharepoint.com/personal/218012048_link_cuhk_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F218012048%5Flink%5Fcuhk%5Fedu%5Fcn%2FDocuments%2FSemanticKITTIC&ga=1) and extract them.
```
./dataset/
â”œâ”€â”€ 
â”œâ”€â”€ ...
â””â”€â”€ SemanticKitti/
    â”œâ”€â”€sequences
    â”œâ”€â”€SemanticKITTI-C
        â”œâ”€â”€ clean_data/           
        â”œâ”€â”€ dense_16beam/           
        â”‚   â”œâ”€â”€ velodyne/	
        |   |	â”œâ”€â”€ 000000.bin
        |   |	â”œâ”€â”€ 000001.bin
        |   |	â””â”€â”€ ...
        â”‚   â””â”€â”€ labels/ 
        |   |   â”œâ”€â”€ 000000.label
        |   |   â”œâ”€â”€ 000001.label
        |   |   â””â”€â”€ ...
	    ...
```
You can run the robustness evaluation with
```shell script
cd <root dir of this repo>
python robust_test.py --config config/2DPASS-semantickitti.yaml --gpu 0  --num_vote 12 --checkpoint <dir for the pytorch checkpoint>
```

## Model Zoo
You can download the models with the scores below from [this Google drive folder](https://drive.google.com/drive/folders/1Xy6p_h827lv8J-2iZU8T6SLFkxfoXPBE?usp=sharing).
### SemanticKITTI
|Model (validation)|mIoU (vanilla)|mIoU (TTA)|Parameters|
|:---:|:---:|:---:|:---:|
|MinkowskiNet|65.1%|67.1%|21.7M|
|SPVCNN|65.9%|67.8%|21.8M|
|2DPASS (4scale-64dimension)|68.7%|70.0%|1.9M|
|2DPASS (6scale-256dimension)|70.7%|72.0%|45.6M|

Here, we fine-tune 2DPASS models on SemanticKITTI with more epochs and thus gain the higher mIoU. If you train with 64 epochs, it should be gained about 66%/69% for vanilla and 69%/71% after TTA.

### NuScenes
|Model (validation)|mIoU (vanilla)|mIoU (TTA)|Parameters|
|:---:|:---:|:---:|:---:|
|MinkowskiNet|74.3%|76.0%|21.7M|
|SPVCNN|74.9%|76.9%|21.8M|
|2DPASS (6scale-128dimension)|76.7%|79.6%|11.5M|
|2DPASS (6scale-256dimension)|78.0%|80.5%|45.6M|

**Note that the results on benchmarks are gained by training with additional validation set and using instance-level augmentation.**

## Acknowledgements
Code is built based on [SPVNAS](https://github.com/mit-han-lab/spvnas), [Cylinder3D](https://github.com/xinge008/Cylinder3D), [xMUDA](https://github.com/valeoai/xmuda) and [SPCONV](https://github.com/traveller59/spconv).

## License
This repository is released under MIT License (see LICENSE file for details).


å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† 2D å…ˆéªŒè¾…åŠ©è¯­ä¹‰åˆ†å‰²ï¼ˆ2DPASSï¼‰æ–¹æ³•ï¼Œä¸€ç§é€šç”¨è®­ç»ƒæ–¹æ¡ˆï¼Œç”¨äºä¿ƒè¿›ç‚¹äº‘ä¸Šçš„è¡¨ç¤ºå­¦ä¹ ã€‚æ‰€æå‡ºçš„2DPASæ–¹æ³•å……åˆ†åˆ©ç”¨äº†è®­ç»ƒè¿‡ç¨‹ä¸­ä¸°å¯Œçš„2Då›¾åƒï¼Œç„¶ååœ¨æ²¡æœ‰ä¸¥æ ¼é…å¯¹æ•°æ®çº¦æŸçš„æƒ…å†µä¸‹è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚
åœ¨å®è·µä¸­ï¼Œé€šè¿‡åˆ©ç”¨è¾…åŠ©æ¨¡æ€èåˆå’Œå¤šå°ºåº¦èåˆåˆ°å•ä¸€çŸ¥è¯†è’¸é¦ ï¼ˆMSFSKDï¼‰ï¼Œ2DPASS ä»å¤šæ¨¡æ€æ•°æ®ä¸­è·å–æ›´ä¸°å¯Œçš„è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ï¼Œç„¶åå°†å…¶æç‚¼åˆ°çº¯ 3D ç½‘ç»œä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„åŸºçº¿æ¨¡å‹åœ¨é…å¤‡ 2DPASS åä»…ä½¿ç”¨ç‚¹äº‘è¾“å…¥å³å¯è·å¾—æ˜¾ç€æ”¹è¿›ã€‚

```
python main.py --config checkpoint/2DPASS-semantickitti.yaml --gpu 0 --test --num_vote 1 --checkpoint ./checkpoint/best_model.ckpt

please install torchsparse if you want to run spvcnn/minkowskinet!
{'format_version': 1, 'model_params': {'model_architecture': 'arch_2dpass', 'input_dims': 4, 'spatial_shape': [1000, 1000, 60], 'scale_list': [2, 4, 8, 16], 'hiden_size': 64, 'num_classes': 20, 'backbone_2d': 'resnet34', 'pretrained2d': False}, 'dataset_params': {'training_size': 19132, 'dataset_type': 'point_image_dataset_semkitti', 'pc_dataset_type': 'SemanticKITTI', 'collate_type': 'collate_fn_default', 'ignore_label': 0, 'label_mapping': './config/label_mapping/semantic-kitti.yaml', 'bottom_crop': [480, 320], 'color_jitter': [0.4, 0.4, 0.4], 'flip2d': 0.5, 'image_normalizer': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'max_volume_space': [50, 50, 2], 'min_volume_space': [-50, -50, -4], 'seg_labelweights': [0, 55437630, 320797, 541736, 2578735, 3274484, 552662, 184064, 78858, 240942562, 17294618, 170599734, 6369672, 230413074, 101130274, 476491114, 9833174, 129609852, 4506626, 1168181], 'train_data_loader': {'data_path': './dataset/SemanticKitti/dataset/sequences/', 'batch_size': 8, 'shuffle': True, 'num_workers': 8, 'rotate_aug': True, 'flip_aug': True, 'scale_aug': True, 'transform_aug': True, 'dropout_aug': True}, 'val_data_loader': {'data_path': './dataset/SemanticKitti/dataset/sequences/', 'shuffle': False, 'num_workers': 8, 'batch_size': 1, 'rotate_aug': False, 'flip_aug': False, 'scale_aug': False, 'transform_aug': False, 'dropout_aug': False}}, 'train_params': {'max_num_epochs': 64, 'learning_rate': 0.24, 'optimizer': 'SGD', 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0001, 'lambda_seg2d': 1, 'lambda_xm': 0.05}, 'gpu': [0], 'seed': 0, 'config_path': 'checkpoint/2DPASS-semantickitti.yaml', 'log_dir': 'default', 'monitor': 'val/mIoU', 'stop_patience': 50, 'save_top_k': 1, 'check_val_every_n_epoch': 1, 'SWA': False, 'baseline_only': False, 'test': True, 'fine_tune': False, 'pretrain2d': False, 'num_vote': 1, 'submit_to_server': False, 'checkpoint': 'checkpoint/best_model.ckpt', 'debug': False}
Global seed set to 0
load pre-trained model...
Start testing...
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Global seed set to 0
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Validation per class iou:                                                                                                                                                                                                             
car : 96.83%
bicycle : 52.55%
motorcycle : 76.33%
truck : 90.74%
bus : 71.38%
person : 78.36%
bicyclist : 92.35%
motorcyclist : 0.06%
road : 93.24%
parking : 50.75%
sidewalk : 80.08%
other-ground : 8.44%
building : 92.21%
fence : 68.27%
vegetation : 88.37%
trunk : 71.19%
terrain : 74.63%
pole : 63.92%
traffic-sign : 53.46%
Current val miou is 68.587 while the best val miou is 68.587
Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4071/4071 [06:19<00:00, 10.72it/s]
--------------------------------------------------------------------------------
DATALOADER:0 TEST RESULTS
{'val/acc': 0.8935943841934204,
 'val/best_miou': 0.6858724848558865,
 'val/mIoU': 0.6858724848558865}
--------------------------------------------------------------------------------
```

2dpass æ¨¡å‹ç»“æ„
model_3d æ˜¯ SPVCNN
model_2d æ˜¯ ResNetFCN
fusion æ˜¯ xModalKD

data_dict -> model_3d -> model_2d -> fusion -> data_dict  ??

### data_dict
/home/bairui/program/2dpass/network/arch_2dpass.py->171: points   points[0].shape: torch.Size([119549, 4])  [xyz, sig]
/home/bairui/program/2dpass/network/arch_2dpass.py->171: ref_xyz   ref_xyz[0].shape: torch.Size([119549, 3])
/home/bairui/program/2dpass/network/arch_2dpass.py->171: batch_idx   batch_idx.shape: torch.Size([119549]) æ¯ä¸ªç‚¹å±äºbatchä¸­çš„ç¬¬å‡ å¸§ç‚¹äº‘
/home/bairui/program/2dpass/network/arch_2dpass.py->171: batch_size
/home/bairui/program/2dpass/network/arch_2dpass.py->171: labels  labels.shape: torch.Size([119549, 1])
/home/bairui/program/2dpass/network/arch_2dpass.py->171: raw_labels   raw_labels.shape: (123389, 1)
/home/bairui/program/2dpass/network/arch_2dpass.py->171: origin_len   origin_len: 123389
/home/bairui/program/2dpass/network/arch_2dpass.py->171: indices
/home/bairui/program/2dpass/network/arch_2dpass.py->171: point2img_index  point2img_index[0].shape: torch.Size([8543])  ä¸€ä¸ªlistï¼Œbatchä¸­æ¯å¸§ç‚¹äº‘å¯ä»¥æŠ•å½±åˆ°å›¾åƒèŒƒå›´å†…çš„ç‚¹äº‘ç´¢å¼•  
/home/bairui/program/2dpass/network/arch_2dpass.py->171: img  img[0].shape: torch.Size([320, 480, 3])
/home/bairui/program/2dpass/network/arch_2dpass.py->171: img_indices  img_indices[0].shape: (8543, 2)  batchä¸­æ¯å¸§ç‚¹äº‘æŠ•å½±åˆ°å›¾åƒçš„åƒç´ åæ ‡
/home/bairui/program/2dpass/network/arch_2dpass.py->171: img_label  img_label.shape: torch.Size([8543, 1])   æ ¹æ®ç‚¹äº‘.labelï¼Œä»¥åŠæŠ•å½±å…³ç³»å¾—åˆ°çš„æŠ•å½±åˆ°å›¾åƒå¹³é¢çš„ç‚¹çš„æ ‡ç­¾
/home/bairui/program/2dpass/network/arch_2dpass.py->171: path  path: ['/home/bairui/program/2dpass/dataset/SemanticKitti/dataset/sequences/08/velodyne/000000.bin']

/home/bairui/program/2dpass/dataloader/dataset.py->182: keep_idx_img_pts: [ True  True  True ... False False False]
keep_idx [true, false ...] åŒ…å«æ˜¯å¦ç¬¦åˆæ¡ä»¶çš„ç‚¹äº‘ç´¢å¼•

data_dict['points'].shape: torch.Size([119549, 4])
pc.shape: torch.Size([119549, 3])
self.scale_list: [2, 4, 8, 16, 1]
self.coors_range_xyz: [[-50, 50], [-50, 50], [-4, 2]]
self.spatial_shape: [1000 1000   60]
xidx.shape: torch.Size([119549])  å¾—åˆ°æ¯ä¸ªç‚¹åœ¨ä¸åŒscaleä½“ç´ ä¸­çš„åæ ‡


```
data_dict['scale_{}'.format(scale)] = {
                'full_coors': bxyz_indx,
                'coors_inv': unq_inv,  æ¯ä¸ªç‚¹æ‰€å±çš„ä½“ç´ ç¼–å·
                'coors': unq.type(torch.int32)   ä¸åŒçš„scaleå¯¹åº”çš„unqä¸åŒ, scaleè¶Šå¤§ï¼Œ å»é™¤é‡å¤åçš„bzyx
            }
```
**scale_1**
/home/bairui/program/2dpass/network/voxel_fea_generator.py->50: full_coors: bxyz_indx.shape: torch.Size([119549, 4])  å¯¹äºä¸åŒscaleï¼Œæ¯ä¸ªç‚¹åœ¨ä½“ç´ ä¸­çš„åæ ‡(ç´¢å¼•)
/home/bairui/program/2dpass/network/voxel_fea_generator.py->54: scale_1 coors_inv: unq_inv.shape: torch.Size([119549]) æ¯ä¸ªç‚¹åœ¨ä¸åŒscaleå¯¹åº”çš„ä½“ç´ ä¸­çš„ç´¢å¼•
/home/bairui/program/2dpass/network/voxel_fea_generator.py->55: scale_1 coors: unq.shape: torch.Size([64811, 4]) ä¸é‡å¤çš„ä½“ç´ åæ ‡å³éç©ºçš„ä½“ç´ åæ ‡

/home/bairui/program/2dpass/network/voxel_fea_generator.py->43: ğŸš€scale: 1
/home/bairui/program/2dpass/network/voxel_fea_generator.py->47: xidx.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->51: bxyz_indx.shape: torch.Size([119549, 4])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->55: scale_1 unq_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->56: scale_1 unq.shape: torch.Size([64811, 4])

**scale_2**
full_coors: bxyz_indx.shape: torch.Size([119549, 4])  æ¯ä¸ªç‚¹åœ¨ä½“ç´ ä¸­çš„åæ ‡(ç´¢å¼•)
coors_inv: unq_inv.shape: torch.Size([119549])  æ¯ä¸ªç‚¹æ‰€å±çš„ä½“ç´ ç¼–å·
coors: unq.shape: torch.Size([38399, 4])   ä¸åŒçš„scaleå¯¹åº”çš„unqä¸åŒ, scaleè¶Šå¤§ï¼Œ å»é™¤é‡å¤åçš„bzyx

/home/bairui/program/2dpass/network/voxel_fea_generator.py->43: ğŸš€scale: 2
/home/bairui/program/2dpass/network/voxel_fea_generator.py->47: xidx.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->51: bxyz_indx.shape: torch.Size([119549, 4])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->55: scale_2 unq_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->56: scale_2 coors: unq.shape: torch.Size([38399, 4])

**scale_4**
/home/bairui/program/2dpass/network/voxel_fea_generator.py->43: ğŸš€scale: 4
/home/bairui/program/2dpass/network/voxel_fea_generator.py->47: xidx.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->51: bxyz_indx.shape: torch.Size([119549, 4])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->55: scale_4 unq_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->56: scale_4 unq.shape: torch.Size([18400, 4])

**scale_8**
/home/bairui/program/2dpass/network/voxel_fea_generator.py->43: ğŸš€scale: 8
/home/bairui/program/2dpass/network/voxel_fea_generator.py->47: xidx.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->51: bxyz_indx.shape: torch.Size([119549, 4])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->55: scale_8 unq_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->56: scale_8 unq.shape: torch.Size([7757, 4])

**scale_16**
/home/bairui/program/2dpass/network/voxel_fea_generator.py->43: ğŸš€scale: 16
/home/bairui/program/2dpass/network/voxel_fea_generator.py->47: xidx.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->51: bxyz_indx.shape: torch.Size([119549, 4])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->55: scale_16 unq_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->56: scale_16 unq.shape: torch.Size([2881, 4])

**scale_1**
/home/bairui/program/2dpass/network/voxel_fea_generator.py->43: ğŸš€scale: 1
/home/bairui/program/2dpass/network/voxel_fea_generator.py->47: xidx.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->51: bxyz_indx.shape: torch.Size([119549, 4])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->55: scale_1 unq_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->56: scale_1 unq.shape: torch.Size([64811, 4])


### data_dict -> model_3d.voxelizer -> data_dict

/home/bairui/program/2dpass/network/baseline.py->174: points
/home/bairui/program/2dpass/network/baseline.py->174: ref_xyz
/home/bairui/program/2dpass/network/baseline.py->174: batch_idx
/home/bairui/program/2dpass/network/baseline.py->174: batch_size
/home/bairui/program/2dpass/network/baseline.py->174: labels
/home/bairui/program/2dpass/network/baseline.py->174: raw_labels
/home/bairui/program/2dpass/network/baseline.py->174: origin_len
/home/bairui/program/2dpass/network/baseline.py->174: indices
/home/bairui/program/2dpass/network/baseline.py->174: point2img_index  
/home/bairui/program/2dpass/network/baseline.py->174: img
/home/bairui/program/2dpass/network/baseline.py->174: img_indices
/home/bairui/program/2dpass/network/baseline.py->174: img_label
/home/bairui/program/2dpass/network/baseline.py->174: path
/home/bairui/program/2dpass/network/baseline.py->174: scale_2   ä¸åŒscaleå¯¹åº”çš„voxelåŒ–åæ¯ä¸ªç‚¹çš„åæ ‡
/home/bairui/program/2dpass/network/baseline.py->174: scale_4
/home/bairui/program/2dpass/network/baseline.py->174: scale_8
/home/bairui/program/2dpass/network/baseline.py->174: scale_16
/home/bairui/program/2dpass/network/baseline.py->174: scale_1


### data_dict -> model.voxel_3d_generator -> data_dict

self.coors_range_xyz: [[-50, 50], [-50, 50], [-4, 2]]
self.spatial_shape: [1000 1000   60]
/home/bairui/program/2dpass/network/baseline.py->141: out_channels: hiden_size: 64
/home/bairui/program/2dpass/network/baseline.py->142: in_channels: input_dims: 4

intervals: tensor([0.1000, 0.1000, 0.1000], device='cuda:0')
grid_ind.shape: torch.Size([119549, 3])

/home/bairui/program/2dpass/network/voxel_fea_generator.py->91: point.shape: torch.Size([119549, 4])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->92: nor_pc.shape: torch.Size([119549, 3])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->93: center_to_point: torch.Size([119549, 3])
/home/bairui/program/2dpass/network/voxel_fea_generator.py->96: pc_feature.shape: torch.Size([119549, 10])  åŒ…å«ç‚¹äº‘åæ ‡å’Œå¼ºåº¦[0, 4], åŒä¸€ä½“ç´ ç‚¹äº‘çš„å‡å€¼[5, 7], ç‚¹äº‘ä¸æ‰€å±ä½“ç´ ä¸­å¿ƒçš„åç§»[8, 10]

**pt_fea = self.PPmodel(pt_fea)**
/home/bairui/program/2dpass/network/voxel_fea_generator.py->106: pt_fea.shape: torch.Size([119549, 64])

**å¯¹å¤„äºç›¸åŒä½“ç´ çš„æå–åçš„ç‰¹å¾æ±‚mean**
/home/bairui/program/2dpass/network/voxel_fea_generator.py->109: feature.shape: torch.Size([64811, 64])

**data_dict['sparse_tensor']**
spconv.SparseConvTensor(feature=torch.Size([64811, 64]), indices=coors: unq.shape: torch.Size([64811, 4]) (batch_idx, z, y, x), spatial_shape=[60, 1000, 1000], batch_size)

/home/bairui/program/2dpass/network/baseline.py->182: points
/home/bairui/program/2dpass/network/baseline.py->182: ref_xyz
/home/bairui/program/2dpass/network/baseline.py->182: batch_idx
/home/bairui/program/2dpass/network/baseline.py->182: batch_size
/home/bairui/program/2dpass/network/baseline.py->182: labels
/home/bairui/program/2dpass/network/baseline.py->182: raw_labels
/home/bairui/program/2dpass/network/baseline.py->182: origin_len
/home/bairui/program/2dpass/network/baseline.py->182: indices
/home/bairui/program/2dpass/network/baseline.py->182: point2img_index
/home/bairui/program/2dpass/network/baseline.py->182: img
/home/bairui/program/2dpass/network/baseline.py->182: img_indices
/home/bairui/program/2dpass/network/baseline.py->182: img_label
/home/bairui/program/2dpass/network/baseline.py->182: path
/home/bairui/program/2dpass/network/baseline.py->182: scale_2
/home/bairui/program/2dpass/network/baseline.py->182: scale_4
/home/bairui/program/2dpass/network/baseline.py->182: scale_8
/home/bairui/program/2dpass/network/baseline.py->182: scale_16
/home/bairui/program/2dpass/network/baseline.py->182: scale_1
/home/bairui/program/2dpass/network/baseline.py->182: sparse_tensor
/home/bairui/program/2dpass/network/baseline.py->182: coors       è§ä¸Šæ–¹scale_1[coors]
/home/bairui/program/2dpass/network/baseline.py->182: coors_inv  == ['scale_1']['coors_inv']
/home/bairui/program/2dpass/network/baseline.py->182: full_coors == ['scale_1']['full_coors']


### data_dict -> self.spv_enc(encoder: SPVBlock) -> data_dict
**spvblock**
in/out_channels = 64
indice_key = spv_0/1/2/3
scale = 2/4/8/16
last_scale = 1/2/4/8
spatial_shape: [60, 1000, 1000] / [30, 500, 500] / [15, 250, 250] / [7, 125, 125]  

/home/bairui/program/2dpass/network/baseline.py->113: layer_0['pts_feat'].shape: torch.Size([64811, 64])  ä¸åŒscaleçš„è¿›è¡Œspconvåçš„ç‚¹äº‘ç‰¹å¾
/home/bairui/program/2dpass/network/baseline.py->114: layer_0['full_coors'].shape: torch.Size([119549, 4]) point_encoderé€æ¸ä»å¯¹åº”scale_iä¸­è¯»å‡ºfull_coorsä¸­ï¼Œlayer_iä¸­è®¾ç½®ä¸ºdata_dict['full_coors'], å› æ­¤layer_i(0, 1, 2)å¯¹åº”scale_i(2, 4, 8, 16)çš„full_coors(å³ä¸åŒscaleç‚¹äº‘ä½“ç´ åŒ–åï¼Œæ¯ä¸ªç‚¹æ‰€åœ¨ä½“ç´ çš„åæ ‡)

/home/bairui/program/2dpass/network/baseline.py->113: layer_1['pts_feat'].shape: torch.Size([38399, 64])
/home/bairui/program/2dpass/network/baseline.py->114: layer_1['full_coors'].shape: torch.Size([119549, 4])

/home/bairui/program/2dpass/network/baseline.py->113: layer_2['pts_feat'].shape: torch.Size([18400, 64])
/home/bairui/program/2dpass/network/baseline.py->114: layer_2['full_coors'].shape: torch.Size([119549, 4])

/home/bairui/program/2dpass/network/baseline.py->113: layer_3['pts_feat'].shape: torch.Size([7757, 64])
/home/bairui/program/2dpass/network/baseline.py->114: layer_3['full_coors'].shape: torch.Size([119549, 4])


point_encoder:
**downsample**  # å¯¹éç©ºä½“ç´ å¯¹åº”çš„ç‰¹å¾è¿›è¡Œ2å€ä¸‹é‡‡æ ·
/home/bairui/program/2dpass/network/baseline.py->57: features.size: torch.Size([64811, 64])
/home/bairui/program/2dpass/network/baseline.py->58: data_dict['coors']: torch.Size([64811, 4])
ä¸‹é‡‡æ ·å
/home/bairui/program/2dpass/network/baseline.py->61: output: torch.Size([38399, 64])

/home/bairui/program/2dpass/network/baseline.py->57: features.size: torch.Size([64811, 64])
/home/bairui/program/2dpass/network/baseline.py->58: data_dict['coors']: torch.Size([64811, 4])
/home/bairui/program/2dpass/network/baseline.py->61: output: torch.Size([38399, 64])
/home/bairui/program/2dpass/network/baseline.py->64: identity: torch.Size([64811, 64])
/home/bairui/program/2dpass/network/baseline.py->65: output: torch.Size([64811, 64])
/home/bairui/program/2dpass/network/baseline.py->67: output: torch.Size([64811, 128])
/home/bairui/program/2dpass/network/baseline.py->74: v_feat: torch.Size([38399, 64])


/home/bairui/program/2dpass/network/baseline.py->192: points
/home/bairui/program/2dpass/network/baseline.py->192: ref_xyz
/home/bairui/program/2dpass/network/baseline.py->192: batch_idx
/home/bairui/program/2dpass/network/baseline.py->192: batch_size
/home/bairui/program/2dpass/network/baseline.py->192: labels
/home/bairui/program/2dpass/network/baseline.py->192: raw_labels
/home/bairui/program/2dpass/network/baseline.py->192: origin_len
/home/bairui/program/2dpass/network/baseline.py->192: indices
/home/bairui/program/2dpass/network/baseline.py->192: point2img_index
/home/bairui/program/2dpass/network/baseline.py->192: img
/home/bairui/program/2dpass/network/baseline.py->192: img_indices
/home/bairui/program/2dpass/network/baseline.py->192: img_label
/home/bairui/program/2dpass/network/baseline.py->192: path
/home/bairui/program/2dpass/network/baseline.py->192: scale_2
/home/bairui/program/2dpass/network/baseline.py->192: scale_4
/home/bairui/program/2dpass/network/baseline.py->192: scale_8
/home/bairui/program/2dpass/network/baseline.py->192: scale_16
/home/bairui/program/2dpass/network/baseline.py->192: scale_1
/home/bairui/program/2dpass/network/baseline.py->192: sparse_tensor
/home/bairui/program/2dpass/network/baseline.py->192: coors
/home/bairui/program/2dpass/network/baseline.py->192: coors_inv
/home/bairui/program/2dpass/network/baseline.py->192: full_coors
/home/bairui/program/2dpass/network/baseline.py->192: layer_0
/home/bairui/program/2dpass/network/baseline.py->192: layer_1
/home/bairui/program/2dpass/network/baseline.py->192: layer_2
/home/bairui/program/2dpass/network/baseline.py->192: layer_3

### data_dict -> model_3d -> data_dict
/home/bairui/program/2dpass/network/arch_2dpass.py->172: points
/home/bairui/program/2dpass/network/arch_2dpass.py->172: ref_xyz
/home/bairui/program/2dpass/network/arch_2dpass.py->172: batch_idx
/home/bairui/program/2dpass/network/arch_2dpass.py->172: batch_size
/home/bairui/program/2dpass/network/arch_2dpass.py->172: labels
/home/bairui/program/2dpass/network/arch_2dpass.py->172: raw_labels
/home/bairui/program/2dpass/network/arch_2dpass.py->172: origin_len
/home/bairui/program/2dpass/network/arch_2dpass.py->172: indices
/home/bairui/program/2dpass/network/arch_2dpass.py->172: point2img_index
/home/bairui/program/2dpass/network/arch_2dpass.py->172: img
/home/bairui/program/2dpass/network/arch_2dpass.py->172: img_indices
/home/bairui/program/2dpass/network/arch_2dpass.py->172: img_label
/home/bairui/program/2dpass/network/arch_2dpass.py->172: path
/home/bairui/program/2dpass/network/arch_2dpass.py->172: scale_2
/home/bairui/program/2dpass/network/arch_2dpass.py->172: scale_4
/home/bairui/program/2dpass/network/arch_2dpass.py->172: scale_8
/home/bairui/program/2dpass/network/arch_2dpass.py->172: scale_16
/home/bairui/program/2dpass/network/arch_2dpass.py->172: scale_1
/home/bairui/program/2dpass/network/arch_2dpass.py->172: sparse_tensor
/home/bairui/program/2dpass/network/arch_2dpass.py->172: coors
/home/bairui/program/2dpass/network/arch_2dpass.py->172: coors_inv
/home/bairui/program/2dpass/network/arch_2dpass.py->172: full_coors
/home/bairui/program/2dpass/network/arch_2dpass.py->172: layer_0
/home/bairui/program/2dpass/network/arch_2dpass.py->172: layer_1
/home/bairui/program/2dpass/network/arch_2dpass.py->172: layer_2
/home/bairui/program/2dpass/network/arch_2dpass.py->172: layer_3
/home/bairui/program/2dpass/network/arch_2dpass.py->172: logits
/home/bairui/program/2dpass/network/arch_2dpass.py->172: loss
/home/bairui/program/2dpass/network/arch_2dpass.py->172: loss_main_ce
/home/bairui/program/2dpass/network/arch_2dpass.py->172: loss_main_lovasz

ä¸åŒscaleçš„ç‰¹å¾æå–åå¯¹åº”æ¯ä¸ªç‚¹çš„ç‰¹å¾
/home/bairui/program/2dpass/network/baseline.py->212: enc_feats[0].shape: torch.Size([119549, 64])
/home/bairui/program/2dpass/network/baseline.py->212: enc_feats[1].shape: torch.Size([119549, 64])
/home/bairui/program/2dpass/network/baseline.py->212: enc_feats[2].shape: torch.Size([119549, 64])
/home/bairui/program/2dpass/network/baseline.py->212: enc_feats[3].shape: torch.Size([119549, 64])
/home/bairui/program/2dpass/network/baseline.py->215: output.shape: torch.Size([119549, 256])
output.shape: torch.Size([119549, 256])
256 = hiden_size * num_class
**å¾—åˆ°logits**
data_dict['logits'] = self.classifier(output)
/home/bairui/program/2dpass/network/baseline.py->218: data_dict['logits'].shape: torch.Size([119549, 20])

**å¾—åˆ°loss**
ce_loss: logits <-> labels
lovasz_loss: softmax(logits) <-> labels


### data_dict -> model_2d -> data_dict
x.shape:torch.Size([1, 3, 320, 480])
conv1_out.shape:torch.Size([1, 64, 320, 480])
**encoder**
/home/bairui/program/2dpass/network/basic_block.py->103: layer1_out.shape:torch.Size([1, 64, 160, 240])
/home/bairui/program/2dpass/network/basic_block.py->104: layer2_out.shape:torch.Size([1, 128, 80, 120])
/home/bairui/program/2dpass/network/basic_block.py->105: layer3_out.shape:torch.Size([1, 256, 40, 60])
/home/bairui/program/2dpass/network/basic_block.py->106: layer4_out.shape:torch.Size([1, 512, 20, 30])

**deconv**
/home/bairui/program/2dpass/network/basic_block.py->116: layer1_out.shape:torch.Size([1, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->117: layer2_out.shape:torch.Size([1, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->118: layer3_out.shape:torch.Size([1, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->119: layer4_out.shape:torch.Size([1, 64, 320, 480])

/home/bairui/program/2dpass/network/basic_block.py->129: process_keys: ['img_scale2', 'img_scale4', 'img_scale8', 'img_scale16']
/home/bairui/program/2dpass/network/basic_block.py->130: img_indices[0]: (8543, 2)
##### å›¾åƒè¿›è¡Œç‰¹å¾æå–å¹¶è¿›è¡Œåå·ç§¯(ä¸Šé‡‡æ ·)åçš„ç‰¹å¾
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale2].shape: torch.Size([1, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale4].shape: torch.Size([1, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale8].shape: torch.Size([1, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale16].shape: torch.Size([1, 64, 320, 480])
##### ç­›é€‰å‡ºå¯ä»¥å’Œç‚¹äº‘æŠ•å½±å¯¹åº”çš„å›¾åƒç‰¹å¾
/home/bairui/program/2dpass/network/basic_block.py->138: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/basic_block.py->138: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/basic_block.py->138: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/basic_block.py->138: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/basic_block.py->143: data_dict[img_scale2].shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/basic_block.py->143: data_dict[img_scale4].shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/basic_block.py->143: data_dict[img_scale8].shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/basic_block.py->143: data_dict[img_scale16].shape: torch.Size([8543, 64])

/home/bairui/program/2dpass/network/arch_2dpass.py->179: points
/home/bairui/program/2dpass/network/arch_2dpass.py->179: ref_xyz
/home/bairui/program/2dpass/network/arch_2dpass.py->179: batch_idx
/home/bairui/program/2dpass/network/arch_2dpass.py->179: batch_size
/home/bairui/program/2dpass/network/arch_2dpass.py->179: labels
/home/bairui/program/2dpass/network/arch_2dpass.py->179: raw_labels
/home/bairui/program/2dpass/network/arch_2dpass.py->179: origin_len
/home/bairui/program/2dpass/network/arch_2dpass.py->179: indices
/home/bairui/program/2dpass/network/arch_2dpass.py->179: point2img_index
/home/bairui/program/2dpass/network/arch_2dpass.py->179: img
/home/bairui/program/2dpass/network/arch_2dpass.py->179: img_indices
/home/bairui/program/2dpass/network/arch_2dpass.py->179: img_label
/home/bairui/program/2dpass/network/arch_2dpass.py->179: path
/home/bairui/program/2dpass/network/arch_2dpass.py->179: scale_2
/home/bairui/program/2dpass/network/arch_2dpass.py->179: scale_4
/home/bairui/program/2dpass/network/arch_2dpass.py->179: scale_8
/home/bairui/program/2dpass/network/arch_2dpass.py->179: scale_16
/home/bairui/program/2dpass/network/arch_2dpass.py->179: scale_1
/home/bairui/program/2dpass/network/arch_2dpass.py->179: sparse_tensor
/home/bairui/program/2dpass/network/arch_2dpass.py->179: coors
/home/bairui/program/2dpass/network/arch_2dpass.py->179: coors_inv
/home/bairui/program/2dpass/network/arch_2dpass.py->179: full_coors
/home/bairui/program/2dpass/network/arch_2dpass.py->179: layer_0
/home/bairui/program/2dpass/network/arch_2dpass.py->179: layer_1
/home/bairui/program/2dpass/network/arch_2dpass.py->179: layer_2
/home/bairui/program/2dpass/network/arch_2dpass.py->179: layer_3
/home/bairui/program/2dpass/network/arch_2dpass.py->179: logits
/home/bairui/program/2dpass/network/arch_2dpass.py->179: loss
/home/bairui/program/2dpass/network/arch_2dpass.py->179: loss_main_ce
/home/bairui/program/2dpass/network/arch_2dpass.py->179: loss_main_lovasz
/home/bairui/program/2dpass/network/arch_2dpass.py->179: img_scale2          torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->179: img_scale4          torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->179: img_scale8          torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->179: img_scale16         torch.Size([8543, 64])


### data_dict -> fusion -> data_dict
labels.shape: torch.Size([119549, 1])

/home/bairui/program/2dpass/network/arch_2dpass.py->104: img_scale2 img_feat.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->105: layer_0 pts_feat.shape: torch.Size([64811, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->106: scale_1 corrs_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/arch_2dpass.py->110: pts_pred_full.shape: torch.Size([64811, 20])
/home/bairui/program/2dpass/network/arch_2dpass.py->81: lbxyz.shape: torch.Size([119549, 5])
/home/bairui/program/2dpass/network/arch_2dpass.py->82: unq_lbxyz.shape: torch.Size([65119, 5])
/home/bairui/program/2dpass/network/arch_2dpass.py->83: count.shape: torch.Size([65119])
/home/bairui/program/2dpass/network/arch_2dpass.py->84: inv_ind.shape: torch.Size([65119])
/home/bairui/program/2dpass/network/arch_2dpass.py->85: label_ind.shape: torch.Size([64811])
/home/bairui/program/2dpass/network/arch_2dpass.py->87: labels.shape: torch.Size([64811])

**p2img_mapping**
/home/bairui/program/2dpass/network/arch_2dpass.py->72: pts_fea.shape: torch.Size([119549, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->73: p2img_idx[0].shape: torch.Size([8543])
/home/bairui/program/2dpass/network/arch_2dpass.py->74: batch_idx.shape: torch.Size([119549])

/home/bairui/program/2dpass/network/arch_2dpass.py->72: pts_fea.shape: torch.Size([119549, 20])
/home/bairui/program/2dpass/network/arch_2dpass.py->73: p2img_idx[0].shape: torch.Size([8543])
/home/bairui/program/2dpass/network/arch_2dpass.py->74: batch_idx.shape: torch.Size([119549])

/home/bairui/program/2dpass/network/arch_2dpass.py->97: img_scale4 img_feat.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->98: layer_1 pts_feat.shape: torch.Size([38399, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->99: scale_2 corrs_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/arch_2dpass.py->103: pts_pred_full.shape: torch.Size([38399, 20])
/home/bairui/program/2dpass/network/arch_2dpass.py->140: fuse_feat.shape: torch.Size([8543, 64])

/home/bairui/program/2dpass/network/arch_2dpass.py->97: img_scale8 img_feat.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->98: layer_2 pts_feat.shape: torch.Size([18400, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->99: scale_4 corrs_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/arch_2dpass.py->103: pts_pred_full.shape: torch.Size([18400, 20])
/home/bairui/program/2dpass/network/arch_2dpass.py->140: fuse_feat.shape: torch.Size([8543, 64])

/home/bairui/program/2dpass/network/arch_2dpass.py->97: img_scale16 img_feat.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->98: layer_3 pts_feat.shape: torch.Size([7757, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->99: scale_8 corrs_inv.shape: torch.Size([119549])
/home/bairui/program/2dpass/network/arch_2dpass.py->103: pts_pred_full.shape: torch.Size([7757, 20])
/home/bairui/program/2dpass/network/arch_2dpass.py->140: fuse_feat.shape: torch.Size([8543, 64])

/home/bairui/program/2dpass/network/arch_2dpass.py->118: pts_feat.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->120: pts_pred.shape: torch.Size([8543, 20])

/home/bairui/program/2dpass/network/arch_2dpass.py->124: feat_learner.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->126: feat_cat.shape: torch.Size([8543, 128])
**ç»è¿‡self.fcs1/self.fcs2**åå¾—åˆ°èåˆç‰¹å¾
/home/bairui/program/2dpass/network/arch_2dpass.py->130: fuse_feat.shape: torch.Size([8543, 64])

èåˆé¢„æµ‹ç»“æœ
/home/bairui/program/2dpass/network/arch_2dpass.py->134: fuse_pred.shape: torch.Size([8543, 20])

/home/bairui/program/2dpass/network/arch_2dpass.py->161: scale_2: fuse_feat.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->161: scale_4: fuse_feat.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->161: scale_8: fuse_feat.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->161: scale_16: fuse_feat.shape: torch.Size([8543, 64])
/home/bairui/program/2dpass/network/arch_2dpass.py->165: torch.cat(img_seg_feat, 1).shape: torch.Size([8543, 256])
/home/bairui/program/2dpass/network/arch_2dpass.py->167: img_seg_logits.shape: torch.Size([8543, 20])




/home/bairui/program/2dpass/network/arch_2dpass.py->183: points
/home/bairui/program/2dpass/network/arch_2dpass.py->183: ref_xyz
/home/bairui/program/2dpass/network/arch_2dpass.py->183: batch_idx
/home/bairui/program/2dpass/network/arch_2dpass.py->183: batch_size
/home/bairui/program/2dpass/network/arch_2dpass.py->183: labels
/home/bairui/program/2dpass/network/arch_2dpass.py->183: raw_labels
/home/bairui/program/2dpass/network/arch_2dpass.py->183: origin_len
/home/bairui/program/2dpass/network/arch_2dpass.py->183: indices
/home/bairui/program/2dpass/network/arch_2dpass.py->183: point2img_index
/home/bairui/program/2dpass/network/arch_2dpass.py->183: img
/home/bairui/program/2dpass/network/arch_2dpass.py->183: img_indices
/home/bairui/program/2dpass/network/arch_2dpass.py->183: img_label
/home/bairui/program/2dpass/network/arch_2dpass.py->183: path
/home/bairui/program/2dpass/network/arch_2dpass.py->183: scale_2
/home/bairui/program/2dpass/network/arch_2dpass.py->183: scale_4
/home/bairui/program/2dpass/network/arch_2dpass.py->183: scale_8
/home/bairui/program/2dpass/network/arch_2dpass.py->183: scale_16
/home/bairui/program/2dpass/network/arch_2dpass.py->183: scale_1
/home/bairui/program/2dpass/network/arch_2dpass.py->183: sparse_tensor
/home/bairui/program/2dpass/network/arch_2dpass.py->183: coors
/home/bairui/program/2dpass/network/arch_2dpass.py->183: coors_inv
/home/bairui/program/2dpass/network/arch_2dpass.py->183: full_coors
/home/bairui/program/2dpass/network/arch_2dpass.py->183: layer_0
/home/bairui/program/2dpass/network/arch_2dpass.py->183: layer_1
/home/bairui/program/2dpass/network/arch_2dpass.py->183: layer_2
/home/bairui/program/2dpass/network/arch_2dpass.py->183: layer_3
/home/bairui/program/2dpass/network/arch_2dpass.py->183: logits
/home/bairui/program/2dpass/network/arch_2dpass.py->183: loss
/home/bairui/program/2dpass/network/arch_2dpass.py->183: loss_main_ce
/home/bairui/program/2dpass/network/arch_2dpass.py->183: loss_main_lovasz
/home/bairui/program/2dpass/network/arch_2dpass.py->183: img_scale2
/home/bairui/program/2dpass/network/arch_2dpass.py->183: img_scale4
/home/bairui/program/2dpass/network/arch_2dpass.py->183: img_scale8
/home/bairui/program/2dpass/network/arch_2dpass.py->183: img_scale16

### spvnas
Sparse Point-Voxel Convolution

åŸºäºç‚¹çš„æ–¹æ³•point-based methodsåœ¨å¤„ç†éç»“æ„åŒ–æ•°æ®ä¸Šè€—è´¹è¶…è¿‡90%æ—¶é—´
åŸºäºä½“ç´ çš„æ–¹æ³•åœ¨ä½“ç´ åˆ†è¾¨ç‡çš„é€‰å–æ”¶åˆ°å¾ˆå¤§å½±å“
æå‡ºSparse Point-Voxel Convolutionç¨€ç–ç‚¹ä½“ç´ å·ç§¯

1. point-based branch ä¿æŒé«˜åˆ†è¾¨ç‡
2. sparse voxel-based branch ä½¿ç”¨ç¨€ç–å·ç§¯æ¥è·¨ä¸åŒçš„æ„Ÿå—é‡
ä¸¤ä¸ªåˆ†æ”¯é€šè¿‡ç¨€ç–ä½“ç´ åŒ–å’Œåä½“ç´ åŒ–æ¥è¿›è¡Œç»“åˆ

ä½¿ç”¨GPU hash tableæ¥åŠ é€Ÿç¨€ç–voxelåŒ–å’Œåvoxel
spconv ä¸­å·²ç»å®ç°äº†å—ï¼Ÿpytorch_scatter

### 3D Semantic Segmentation with Submanifold(å­æµå½¢) Sparse Convolutional Networks
å¼•å…¥äº†æ–°çš„sparse convolutional operationæ¥æ›´é«˜æ•ˆåœ°å¤„ç†spatially-sparse data
å¹¶æ²¡æœ‰æ‰©å±•sparseæ•°æ®ï¼Œè€Œæ˜¯åœ¨æ•´ä¸ªç½‘ç»œä¸­ä¿æŒç›¸åŒçš„ç¨€ç–æ€§ï¼Œè¿™æ ·å¯ä»¥æ­å»ºæ›´å¤šå±‚çš„ç½‘ç»œã€‚
submanifold sparse convolution 
Oct-trees/Kd-trees

d-dimensional convolutional network
å¸¸è§„å·ç§¯æ“ä½œå¹¶æ²¡æœ‰é€‚åº”å…·æœ‰ç¨€ç–ç‰¹ç‚¹çš„ç‰¹å¾
(d + 1)-dimenional tensor -> d-dimensional convolutional network
d-dimçš„siteï¼Œæ¯ä¸ªå¯¹åº”ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚å¯¹äºé0çš„ç‰¹å¾ï¼Œsiteå®šä¹‰ä¸ºactivate
ç›¸å¯¹d-dimçš„è¾“å…¥ï¼Œå¢åŠ 1ç»´æ¥æ ‡è®°å½“å‰ç‰¹å¾æ˜¯å¦ä¸ºactivate(å¯ä»¥æ ¹æ®é˜ˆå€¼æ¥è®¾ç½®)
æ¯ä¸€å±‚çš„æ´»åŠ¨çŠ¶æ€å†³å®šä¸‹ä¸€å±‚æ˜¯å¦æ´»åŠ¨ã€‚éæ´»åŠ¨çš„ç‰¹å¾å‘é‡éƒ½ä¿æŒground stateã€‚å› æ­¤åœ¨è®­ç»ƒæ—¶ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­åªéœ€è®¡ç®—ä¸€æ¬¡ï¼Œè€Œæµ‹è¯•æ—¶ï¼Œå¯¹æ‰€æœ‰æ­£å‘ä¼ æ’­åªéœ€è®¡ç®—ä¸€æ¬¡ï¼ŒèŠ‚çœè®¡ç®—å’Œå†…å­˜ä½¿ç”¨ã€‚
åœ¨å¤šå±‚å·ç§¯ç½‘ç»œä¸­ï¼Œ1ä¸ªactivate siteçš„å·ç§¯ï¼Œ1 activate -> 3 activate -> 5 activate å³å­˜åœ¨æ‰©å¼ é—®é¢˜
**ç¨€ç–æ€§åœ¨å¸¸è§„å·ç§¯å‡ æ¬¡åä¼šå¾ˆå¿«æ¶ˆå¤±**
è§£å†³submanifold dilationçš„æ–¹æ³•:
**é™åˆ¶å·ç§¯çš„è¾“å‡ºä¸ºä»…åŒ…æ‹¬æ´»åŠ¨è¾“å…¥ç‚¹çš„é›†åˆ**
ä¸Šè¿°æ–¹æ¡ˆå¯èƒ½ä½¿ç½‘ç»œä¸­çš„éšè—å±‚ä¸èƒ½æ¥æ”¶è¾“å…¥è¾“å…¥æ•°æ®ä¸­æ‰€æœ‰éœ€è¦åˆ†ç±»çš„ä¿¡æ¯ã€‚ä¸¤ä¸ªç›¸é‚»è¿æ¥çš„ç»„ä»¶è¢«ç‹¬ç«‹å¤„ç†ã€‚
é€šè¿‡ä½¿ç”¨åŒ…å«æ± åŒ–çš„å·ç§¯ç½‘ç»œæˆ–è·¨æ­¥å·ç§¯æ“ä½œæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å®ƒä»¬å…è®¸ä¿¡æ¯åœ¨è¾“å…¥ä¸­æ–­å¼€è¿æ¥çš„ç»„ä»¶ä¹‹é—´æµåŠ¨ã€‚ï¼Ÿï¼Ÿ
ç©ºé—´ä¸Šè¶Šæ¥è¿‘çš„ç»„ä»¶æ‰€éœ€è¦çš„è·¨æ­¥æ“ä½œè¶Šå°‘ã€‚

sparse convolutinal operation
sparse convolution SC(m, n, f, s), m input feature planes, n output feature planes, a filter size of f, stride s.
æŸ¥æ‰¾æ„Ÿå—é‡ä¸­æ˜¯å¦æœ‰activate site. è‹¥inputçš„sizeæ˜¯lï¼Œoutputçš„sizeæ˜¯(l - f + s) / s. SCä¸¢å¼ƒä½äºground stateçš„éæ´»è·ƒinput. å¯ä»¥å¤§å¤§å‡å°‘è®¡ç®—æˆæœ¬ã€‚

Submanifold sparse convolution
SSC(m, n, f)
å‡è®¾fä¸ºä¸€ä¸ªå¥‡æ•°, å°†SC(m, n, f, s = 1)ä¿®æ”¹ä¸ºSSCã€‚
1. pad the input with (f - 1) / 2 zeros on each sideï¼Œso that the output will have the same size as the input.
2. restrict an output site to be activate if the site at the corresponding site in the input is activate.(if the central site in the receptive field is activate)
3. compute the output feature vector which output site is activate

**BN**å¸¸è§„BNä»…åº”ç”¨äºå¤„äºactivateçŠ¶æ€çš„ä½ç½®ã€‚
Max-pooling MP(f, s)å’Œaverage-pooling AP(f, s)å®šä¹‰ä¸ºSC(., ., f, s)çš„å˜ç§ã€‚
å®šä¹‰åå·ç§¯deconvolution DC(., ., f, s)ä½œä¸ºSCçš„é€†

å°†è¾“å…¥/éšè—å±‚çš„çŠ¶æ€å­˜å‚¨ä¸º2ä¸ªéƒ¨åˆ†, ä¸€ä¸ªhashè¡¨å’Œä¸€ä¸ªçŸ©é˜µã€‚çŸ©é˜µ[a, m]åŒ…æ‹¬aä¸ªæ´»åŠ¨sitesï¼Œæ¯è¡Œè¡¨ç¤ºä¸€ä¸ªã€‚
hashè¡¨åŒ…å«(location, row)å¯¹ï¼Œè¡¨ç¤ºæ‰€æœ‰çš„æ´»åŠ¨sitesï¼Œlocationè¡¨ç¤ºæ•´æ•°åæ ‡ï¼Œrowè¡¨ç¤ºç‰¹å¾çŸ©é˜µå¯¹åº”è¡Œ
ä½¿ç”¨gpuæ¥åŠ é€Ÿspconvï¼Œæœ¬è´¨ä¸Šä¹Ÿæ˜¯çŸ©é˜µä¹˜åŠ 


**ä¸ºå•¥åœ¨best_model.ckptä¸Šfine tune ä¼šä½¿å¾—mIoUä¸‹é™ ?**
**é—®é¢˜å·²ç»è§£å†³ï¼Œå› ä¸ºåˆå§‹è®¾ç½®lrå¤ªå¤§ï¼Œè®¾ç½®ä¸º1e-5ä¹‹åfine-tuneå³å¯**

python main.py --log_dir 2DPASS_semkitti --config=./config/2DPASS-semantickitti.yaml --gpu 0 --save_top_k -1 --every_n_train_steps 500 --checkpoint=./checkpoint/best_model.ckpt
#### 0626 test

```
(2dpass) bairui@LAPTOP-NA9RUBN7:~/program/2dpass$ python main.py --config checkpoint/2DPASS-semantickitti.yaml --gpu 0 --test --num_vote 1 --checkpoint ./logs/SemanticKITTI/2DPASS_semkitti/version_10/checkpoints/last.ckpt
please install torchsparse if you want to run spvcnn/minkowskinet!
{'format_version': 1, 'model_params': {'model_architecture': 'arch_2dpass', 'input_dims': 4, 'spatial_shape': [1000, 1000, 60], 'scale_list': [2, 4, 8, 16], 'hiden_size': 64, 'num_classes': 20, 'backbone_2d': 'resnet34', 'pretrained2d': False}, 'dataset_params': {'training_size': 19132, 'dataset_type': 'point_image_dataset_semkitti', 'pc_dataset_type': 'SemanticKITTI', 'collate_type': 'collate_fn_default', 'ignore_label': 0, 'label_mapping': './config/label_mapping/semantic-kitti.yaml', 'bottom_crop': [480, 320], 'color_jitter': [0.4, 0.4, 0.4], 'flip2d': 0.5, 'image_normalizer': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'max_volume_space': [50, 50, 2], 'min_volume_space': [-50, -50, -4], 'seg_labelweights': [0, 55437630, 320797, 541736, 2578735, 3274484, 552662, 184064, 78858, 240942562, 17294618, 170599734, 6369672, 230413074, 101130274, 476491114, 9833174, 129609852, 4506626, 1168181], 'train_data_loader': {'data_path': './dataset/SemanticKitti/dataset/sequences/', 'batch_size': 8, 'shuffle': True, 'num_workers': 8, 'rotate_aug': True, 'flip_aug': True, 'scale_aug': True, 'transform_aug': True, 'dropout_aug': True}, 'val_data_loader': {'data_path': './dataset/SemanticKitti/dataset/sequences/', 'shuffle': False, 'num_workers': 8, 'batch_size': 1, 'rotate_aug': False, 'flip_aug': False, 'scale_aug': False, 'transform_aug': False, 'dropout_aug': False}}, 'train_params': {'max_num_epochs': 64, 'learning_rate': 0.24, 'optimizer': 'SGD', 'lr_scheduler': 'CosineAnnealingWarmRestarts', 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0001, 'lambda_seg2d': 1, 'lambda_xm': 0.05}, 'gpu': [0], 'seed': 0, 'config_path': 'checkpoint/2DPASS-semantickitti.yaml', 'log_dir': 'default', 'monitor': 'val/mIoU', 'stop_patience': 50, 'save_top_k': 1, 'check_val_every_n_epoch': 1, 'SWA': False, 'baseline_only': False, 'every_n_train_steps': 100, 'test': True, 'fine_tune': False, 'pretrain2d': False, 'num_vote': 1, 'submit_to_server': False, 'checkpoint': './logs/SemanticKITTI/2DPASS_semkitti/version_10/checkpoints/last.ckpt', 'debug': False}
Global seed set to 0
load pre-trained model...
Start testing...
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Global seed set to 0
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Validation per class iou:
car : 85.39%
bicycle : 36.98%
motorcycle : 40.46%
truck : 0.00%
bus : 19.62%
person : 51.02%
bicyclist : 66.21%
motorcyclist : 0.00%
road : 80.95%
parking : 21.19%
sidewalk : 61.26%
other-ground : 0.01%
building : 83.24%
fence : 46.78%
vegetation : 84.27%
trunk : 55.92%
terrain : 66.84%
pole : 52.57%
traffic-sign : 39.70%
Current val miou is 46.969 while the best val miou is 46.969
Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4071/4071 [06:24<00:00, 10.58it/s]
--------------------------------------------------------------------------------
DATALOADER:0 TEST RESULTS
{'val/acc': 0.825825035572052,
 'val/best_miou': 0.46968674243709513,
 'val/mIoU': 0.46968674243709513}
--------------------------------------------------------------------------------
```


å¯è§†åŒ–ä»£ç 
éœ€è¦ææ¸…æ¥šè¾“å…¥å›¾åƒçš„æ ¼å¼ï¼Œå°ºå¯¸, ä½¿ç”¨dummy_image Done


#### 1650 infer test
```
(cylinder3d) br@br-r7000:~/program/2dpass$ python main.py --config config/2DPASS-semantickitti.yaml --gpu 0 --test --num_vote 1 --checkpoint checkpoint/best_model.ckpt 
please install torchsparse if you want to run spvcnn/minkowskinet!
{'format_version': 1, 'model_params': {'model_architecture': 'arch_2dpass', 'input_dims': 4, 'spatial_shape': [1000, 1000, 60], 'scale_list': [2, 4, 8, 16], 'hiden_size': 64, 'num_classes': 20, 'backbone_2d': 'resnet34', 'pretrained2d': False}, 'dataset_params': {'training_size': 19132, 'dataset_type': 'point_image_dataset_semkitti', 'pc_dataset_type': 'SemanticKITTI', 'collate_type': 'collate_fn_default', 'ignore_label': 0, 'label_mapping': './config/label_mapping/semantic-kitti.yaml', 'bottom_crop': [480, 320], 'color_jitter': [0.4, 0.4, 0.4], 'flip2d': 0.5, 'image_normalizer': [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]], 'max_volume_space': [50, 50, 2], 'min_volume_space': [-50, -50, -4], 'seg_labelweights': [0, 55437630, 320797, 541736, 2578735, 3274484, 552662, 184064, 78858, 240942562, 17294618, 170599734, 6369672, 230413074, 101130274, 476491114, 9833174, 129609852, 4506626, 1168181], 'train_data_loader': {'data_path': './dataset/SemanticKitti/dataset/sequences/', 'batch_size': 2, 'shuffle': True, 'num_workers': 2, 'rotate_aug': True, 'flip_aug': True, 'scale_aug': True, 'transform_aug': True, 'dropout_aug': True}, 'val_data_loader': {'data_path': './dataset/SemanticKitti/dataset/sequences/', 'shuffle': False, 'num_workers': 2, 'batch_size': 1, 'rotate_aug': False, 'flip_aug': False, 'scale_aug': False, 'transform_aug': False, 'dropout_aug': False}}, 'train_params': {'max_num_epochs': 64, 'learning_rate': 1e-05, 'optimizer': 'SGD', 'lr_scheduler': 'StepLR', 'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.0001, 'decay_step': 1, 'decay_rate': 0.1, 'lambda_seg2d': 1, 'lambda_xm': 0.05}, 'gpu': [0], 'seed': 0, 'config_path': 'config/2DPASS-semantickitti.yaml', 'log_dir': 'default', 'monitor': 'val/mIoU', 'stop_patience': 50, 'save_top_k': 1, 'check_val_every_n_epoch': 1, 'SWA': False, 'baseline_only': False, 'every_n_train_steps': 3000, 'test': True, 'fine_tune': False, 'pretrain2d': False, 'num_vote': 1, 'submit_to_server': False, 'checkpoint': 'checkpoint/best_model.ckpt', 'debug': False}
Global seed set to 0
load pre-trained model...
Start testing...
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
Global seed set to 0
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Validation per class iou:                                                                                
car : 96.83%
bicycle : 52.55%
motorcycle : 76.33%
truck : 90.74%
bus : 71.38%
person : 78.36%
bicyclist : 92.35%
motorcyclist : 0.06%
road : 93.24%
parking : 50.75%
sidewalk : 80.08%
other-ground : 8.44%
building : 92.21%
fence : 68.27%
vegetation : 88.37%
trunk : 71.19%
terrain : 74.63%
pole : 63.92%
traffic-sign : 53.46%
Current val miou is 68.587 while the best val miou is 68.587
Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4071/4071 [12:35<00:00,  5.39it/s]
--------------------------------------------------------------------------------
DATALOADER:0 TEST RESULTS
{'val/acc': 0.8935943841934204,
 'val/best_miou': 0.6858724991404082,
 'val/mIoU': 0.6858724991404082}
--------------------------------------------------------------------------------
```

`python br_predict_vis.py` å¯è§†åŒ–inferç»“æœ

## TODO
åç»­ç­‰æ–°ç”µè„‘å›æ¥ååœ¨è¶Šé‡æ•°æ®é›†è¿›è¡Œè®­ç»ƒ


## Patchwork
ä¸åŒçš„æ€è·¯, çº¯cppå®ç°ï¼Œæ²¡æœ‰æ·±åº¦å­¦ä¹ æ–¹å¼ï¼Œå…¶å‡ ä½•ç®—æ³•æ˜¯å¦èƒ½æ›´æœ‰æ•ˆçš„è¿›è¡Œç‰¹å¾æå–ï¼Œè¾“å…¥åˆ°ç½‘ç»œä¸­ï¼Ÿ
https://github.com/LimHyungTae/patchwork
https://github.com/url-kaist/patchwork-plusplus
[Patchwork++](http://www.guyuehome.com/38829)

åœ°é¢ç‚¹äº‘åˆ†å‰²ä¸»è¦æ˜¯:
- ä¸ºäº†è§£å†³æ‰¾åˆ°å¯é€šè¡ŒåŒºåŸŸ(movable area), 
- è¿˜å¯ä»¥ç”¨äº**è¯†åˆ«è·Ÿè¸ªç‰©ä½“?**, åˆ†å‰²åœ°é¢ç‚¹äº‘å¯ä»¥èµ·åˆ°é™ä½è®¡ç®—å¤æ‚åº¦(å¤§å¤šæ•°éƒ½æ˜¯åœ°é¢ç‚¹äº‘ï¼Œå¯ä»¥ä½œä¸º**é¢„å¤„ç†é˜¶æ®µ**ï¼Œå…ˆå»é™¤åœ°é¢ç‚¹äº‘ï¼Œé™ä½åç»­è®¡ç®—å¤æ‚åº¦)
é’ˆå¯¹åœ°é¢çš„å‡¹å‡¸ä¸å¹³ä»¥åŠæ–œå¡ç­‰ç±»åˆ« <br>

åˆ†å‰²é€Ÿåº¦è¾¾åˆ°40Hzï¼Œç®—æ³•**é’ˆå¯¹åŸå¸‚ç¯å¢ƒ**

[!patchwork_framework]()
ç®—æ³•ç»“æ„
1. ç‚¹äº‘è¢«ç¼–ç è¿›Concentric Zone Model-based representation(åŸºäºåŒå¿ƒåœ†åŒºåŸŸæ¨¡å‹è¡¨ç¤º)ï¼Œä½¿å¾—ç‚¹äº‘å¯†åº¦åˆ†é…å‡åŒ€ï¼Ÿè®¡ç®—å¤æ‚åº¦ä½(æŒ‡åŒå¿ƒåœ†è¡¨ç¤ºè®¡ç®—)   CZM è¡¨ç¤º
2. ä¹‹åè¿›è¡ŒRegion-wise Ground Plane Fitting(åŒºåŸŸçº§çš„åœ°é¢æ‹Ÿåˆï¼Œ R-GPF)ï¼Œ è¯„ä¼°æ¯ä¸ªåŒºåŸŸçš„åœ°é¢ï¼Ÿ
3. Ground Likelihood Estimation(åœ°é¢ä¼¼ç„¶/å¯èƒ½æ€§ä¼°è®¡ï¼Œ GLE)ï¼Œä»¥å‡å°‘å‡é˜³ç‡  uprightnessç›´ç«‹åº¦ï¼Œelevationé«˜ç¨‹ï¼Œflatnesså¹³æ•´åº¦
åœ°é¢åŒ…æ‹¬ç§»åŠ¨ç‰©ä½“å¯é€šè¡Œçš„åŒºåŸŸï¼Œè‰åœ°ï¼Œäººè¡Œé“ç­‰

- åŸºäºé«˜åº¦è¿‡æ»¤ä»¥åŠRANSACçš„æ–¹æ³•æ— æ³•åˆ†å‰²é™¡å¡ï¼Œé¢ ç°¸ï¼Œä¸å‡åŒ€ï¼Œå‘¨å›´ç‰©ä½“å½±å“æ•ˆæœ
- ç°æœ‰åœ°é¢è¯„ä¼°ç®—æ³•æ—¶æ•ˆæ€§é—®é¢˜ï¼Œä¸é€‚åˆé¢„å¤„ç†
- æ‰«æè¡¨ç¤º(ç‚¹äº‘è¡¨ç¤ºæ–¹å¼ï¼Ÿ)
- elevation map-based 2.5D grid representation åŸºäºé«˜ç¨‹å›¾çš„2.5Dåœ°å›¾è¡¨ç¤º, ç”¨æ¥åŒºåˆ†æ˜¯å¦å±äºåœ°é¢ç‚¹æ¥å°†3Dç‚¹äº‘è¡¨ç¤ºä¸º2.5Dæ ¼å¼. æ— æ³•è¡¨å¾é™¡å¡ï¼Œåœ¨Zå˜åŒ–è¾ƒå¿«æ—¶ï¼Ÿ åˆ°åº•ä»€ä¹ˆæ˜¯2.5D
- æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å®é™…åº”ç”¨
æ—¶éœ€è¦ï¼Œä½¿ç”¨ç¯å¢ƒä¸è®­ç»ƒç¯å¢ƒç›¸è¿‘(å³æ¨¡å‹æ³›åŒ–èƒ½åŠ›)ï¼Œä¼ æ„Ÿå™¨é…ç½®


ç‚¹äº‘è¢«åˆ†ä¸ºä¸¤ç±»åœ°é¢ç‚¹äº‘Gï¼Œ å’Œå‰©ä½™çš„æ‰€æœ‰ç‚¹çš„é›†åˆGc


#### CZM
concentric zone model åŒå¿ƒåœ†æ¨¡å‹
å‡è®¾çœŸå®åœ°é¢å¯ä»¥åœ¨å°èŒƒå›´å†…(small parts)æ˜¯å¹³å¦çš„
é’ˆå¯¹lidaræ•°æ®æœ¬èº«è¿‘å¯†è¿œç–çš„ç‰¹ç‚¹ï¼Œåæ ‡ç³»åˆ’åˆ†å­˜åœ¨è¿œè·ç¦»ç¨€ç–æ€§(ç‚¹äº‘å¤ªç¨€ç–æ— æ³•æ‰¾åˆ°æ¥åœ°å±‚)ï¼Œè¿‘è·ç¦»å­˜åœ¨å¯è¡¨ç¤ºé—®é¢˜(ç½‘æ ¼å¤ªå°)

CZM,ç»™ç½‘æ ¼åˆ†é…äº†åˆé€‚çš„å¯†åº¦å¤§å°ï¼Œåˆ’åˆ†ä¸ºä¸åŒåŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸç”±ä¸åŒå¤§å°çš„bin(ç½‘æ ¼)ç»„æˆã€‚åŒæ—¶è®¡ç®—ä¸å¤æ‚ã€‚
åœ¨è®ºæ–‡ä¸­å°†åŒå¿ƒåœ†åˆ’åˆ†ä¸º4ä¸ªåŒºåŸŸã€‚æ¯ä¸ªåŒºåŸŸåŒ…æ‹¬Nrm * N
æœ€å†…å±‚åŒºåŸŸå’Œæœ€å¤–å±‚åŒºåŸŸçš„ç½‘æ ¼åˆ’åˆ†è¾ƒç¨€ç–ï¼Œæ¥è§£å†³è¿œè·ç¦»ç¨€ç–å’Œè¿‘è·ç¦»å¯è¡¨ç¤ºçš„é—®é¢˜ï¼ŒåŒæ—¶å‡å°‘äº†bin(ç½‘æ ¼)çš„æ•°é‡

#### R-GPF
Region-wise Ground Plane Fitting åŒºåŸŸçº§çš„åœ°é¢æ‹Ÿåˆ
æ¯ä¸ªbiné€šè¿‡R-GPFæ¥è¿›è¡Œä¼°è®¡ï¼Œä¹‹ååˆå¹¶éƒ¨åˆ†åœ°é¢ç‚¹ã€‚ä½¿ç”¨Principal Component Analysis(PCA)ä¸»æˆåˆ†åˆ†æï¼Œç›¸æ¯”RANSACæ›´å¿«(è‡³å°‘2å€)ã€‚

Cæ˜¯ä¸€ä¸ªbinä¸­ç‚¹äº‘çš„åæ–¹å·®çŸ©é˜µï¼Œè®¡ç®—å‡ºCçš„3ä¸ªç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚**å¯¹åº”äºæœ€å°ç‰¹å¾å€¼çš„ç‰¹å¾å‘é‡æ˜¯æœ€æœ‰å¯èƒ½è¡¨ç¤ºå¯¹åº”äºåœ°é¢å±‚çš„æ³•å‘é‡n**ã€‚æ ¹æ®æ³•å‘é‡nå’Œå•ä½ç©ºé—´çš„å¹³å‡ç‚¹è®¡ç®—å¹³é¢ç³»æ•°dã€‚
å°†é«˜åº¦æœ€ä½çš„binä½œä¸ºåœ°è¡¨ã€‚




#### GLE




### TODO

å›¾åƒæ¯å¸§å›¾ç‰‡çš„æ–‡ä»¶åæ ¼å¼éœ€è¦ä¿®

### rellis
image  [1920, 1200]
/home/bairui/program/2dpass/network/basic_block.py->129: process_keys: ['img_scale2', 'img_scale4', 'img_scale8', 'img_scale16']
/home/bairui/program/2dpass/network/basic_block.py->130: img_indices[0].shape: (0, 2)   # ç‚¹äº‘æ ¡å‡†æ–‡ä»¶çš„é—®é¢˜ï¼ŒæŠ•å½±
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale2].shape: torch.Size([2, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale4].shape: torch.Size([2, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale8].shape: torch.Size([2, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale16].shape: torch.Size([2, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->138: img_scale2 : torch.Size([0, 64])
/home/bairui/program/2dpass/network/basic_block.py->138: img_scale4 : torch.Size([0, 64])
/home/bairui/program/2dpass/network/basic_block.py->138: img_scale8 : torch.Size([0, 64])
/home/bairui/program/2dpass/network/basic_block.py->138: img_scale16 : torch.Size([0, 64])

### kitti
[1226, 370]
/home/bairui/program/2dpass/network/basic_block.py->130: img_indices[0].shape: (23287, 2)
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale2].shape: torch.Size([2, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale4].shape: torch.Size([2, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale8].shape: torch.Size([2, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->132: data_dict[img_scale16].shape: torch.Size([2, 64, 320, 480])
/home/bairui/program/2dpass/network/basic_block.py->138: img_scale2 : torch.Size([7039, 64])
/home/bairui/program/2dpass/network/basic_block.py->138: img_scale4 : torch.Size([7039, 64])
/home/bairui/program/2dpass/network/basic_block.py->138: img_scale8 : torch.Size([7039, 64])
/home/bairui/program/2dpass/network/basic_block.py->138: img_scale16 : torch.Size([7039, 64])

rellisçš„æ ¡å‡†æ–‡ä»¶æ˜¯poseçš„æ ¡å‡†ï¼Œè€Œä¸æ˜¯lidar/cameraçš„ä½å§¿å˜æ¢å’Œç›¸æœºå†…å‚
https://github.com/unmannedlab/RELLIS-3D/issues/22
transforms.yaml æ‰æ˜¯cam2lidarçš„å˜æ¢çŸ©é˜µ
## TODO
qæ˜¯å››å…ƒæ•°, tæ˜¯å¹³ç§»å‘é‡
https://www.zhihu.com/tardis/zm/art/78987582?source_id=1005
__å†™ä¸€ä¸ªä»å››å…ƒæ•°åˆ°æ—‹è½¬çŸ©é˜µçš„å‡½æ•°__

Calibration Download:

Camera Instrinsic (Download 2KB)

Basler Camera to Ouster LiDAR (Download 3KB)

Velodyne LiDAR to Ouster LiDAR (Download 3KB)

Stereo Calibration (Download 3KB)

Calibration Raw Data (Download 774MB)

**æ³¨æ„camera to Lidarçš„å˜æ¢çŸ©é˜µéœ€è¦æ±‚é€†ï¼ï¼ï¼ï¼**